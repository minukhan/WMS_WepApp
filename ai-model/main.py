import json

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import tensorflow as tf
import pandas as pd
import numpy as np
import requests
from datetime import datetime, timedelta
import certifi
import os
from dotenv import load_dotenv
from sklearn.metrics import r2_score
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
SERVICE_KEY = os.getenv("SERVICE_KEY")

if not SERVICE_KEY:
    raise ValueError("ğŸš¨ SERVICE_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

# # Load data
# sales_data = pd.read_csv('mock_sales_data.csv')
# exchange_data = pd.read_csv('exchange.csv')
# hyundai_car_stock_data = pd.read_csv('hyundai_stock_monthly_avg.csv')
#
# # Data preprocessing
# sales_data['date'] = pd.to_datetime(sales_data['date'], format='%Y-%m')
# exchange_data['date'] = pd.to_datetime(exchange_data[['Year', 'Month']].assign(Day=1))
# hyundai_car_stock_data['date'] = pd.to_datetime(hyundai_car_stock_data[['Year', 'Month']].assign(Day=1))
#
# # Merge data
# sales_data = sales_data.merge(exchange_data, how='left', on='date')
# sales_data = sales_data.merge(hyundai_car_stock_data, how='left', on='date')
# sales_data.ffill(inplace=True)
#
# # Create features and labels
# sales_data['month'] = sales_data['date'].dt.month
# sales_data['year'] = sales_data['date'].dt.year
#
# X = sales_data[['month', 'year', 'Exchange_rate', 'sales_quantity', 'Hyundai_Month_Stock_Average']]
# y = sales_data['sales_quantity'].shift(-1).dropna()
# X = X[:-1]
#
# # Normalize data
# X_mean, X_std = X.mean(), X.std()
# X_normalized = (X - X_mean) / X_std
# y_mean, y_std = y.mean(), y.std()
# y_normalized = (y - y_mean) / y_std
#
# # TensorFlow model
# model = tf.keras.Sequential([
#     tf.keras.layers.Input(shape=(X_normalized.shape[1],)),  # ì…ë ¥ì¸µ ì •ì˜
#     tf.keras.layers.Dense(16, activation='relu'),
#     tf.keras.layers.Dense(8, activation='relu'),
#     tf.keras.layers.Dense(1)
# ])
#
# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])
# history = model.fit(X_normalized, y_normalized, epochs=30, batch_size=10, verbose=1, validation_split=0.2)
#
#
# # ğŸ”¹ ì˜ˆì¸¡ ìˆ˜í–‰
# predictions_normalized = model.predict(X_normalized)
# predictions = predictions_normalized * y_std + y_mean
#
# mae = tf.keras.losses.MeanAbsoluteError()(y, predictions).numpy()
# mse = tf.keras.losses.MeanSquaredError()(y, predictions).numpy()
#
# print(f"ğŸ“Š Mean Absolute Error (MAE): {mae:.2f}")
# print(f"ğŸ“Š Mean Squared Error (MSE): {mse:.2f}")
# print("ğŸ“Š R2 Score:",r2_score(y,predictions))

# Load data
sales_data = pd.read_csv('mock_sales_data.csv')
exchange_data = pd.read_csv('exchange.csv')
hyundai_car_stock_data = pd.read_csv('hyundai_stock_monthly_avg.csv')

# Data preprocessing
sales_data['date'] = pd.to_datetime(sales_data['date'], format='%Y-%m')
exchange_data['date'] = pd.to_datetime(exchange_data[['Year', 'Month']].assign(Day=1))
hyundai_car_stock_data['date'] = pd.to_datetime(hyundai_car_stock_data[['Year', 'Month']].assign(Day=1))

# Merge data
sales_data = sales_data.merge(exchange_data, how='left', on='date')
sales_data = sales_data.merge(hyundai_car_stock_data, how='left', on='date')
sales_data.ffill(inplace=True)

# Create features and labels
sales_data['month'] = sales_data['date'].dt.month
sales_data['year'] = sales_data['date'].dt.year

# 1. ë¶€í’ˆë³„ ë°ì´í„°ë¡œ ë‚˜ëˆ„ê¸°
part_models = {}

for part_id in sales_data['part_id'].unique():
    part_data = sales_data[sales_data['part_id'] == part_id]

    # Create features and labels
    X_part = part_data[['month', 'year', 'Exchange_rate', 'sales_quantity', 'Hyundai_Month_Stock_Average']]
    y_part = part_data['sales_quantity'].shift(-1).dropna()
    X_part = X_part[:-1]  # Remove the last row to align with y_part

    # Normalize data
    X_mean, X_std = X_part.mean(), X_part.std()
    X_part_normalized = (X_part - X_mean) / X_std
    y_mean, y_std = y_part.mean(), y_part.std()
    y_part_normalized = (y_part - y_mean) / y_std


    # TensorFlow model (ë¶€í’ˆë³„ë¡œ ëª¨ë¸ì„ í•™ìŠµ)
    model_part = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(X_part_normalized.shape[1],)),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.Dense(8, activation='relu'),
        tf.keras.layers.Dense(1)
    ])

    model_part.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])

    # Train-test split
    X_train, X_val, y_train, y_val = train_test_split(X_part_normalized, y_part_normalized, test_size=0.2,
                                                      random_state=42)

    model_part.fit(X_train, y_train, epochs=30, batch_size=10, validation_data=(X_val, y_val), verbose=0)

    # ë¶€í’ˆë³„ ëª¨ë¸ ì €ì¥
    part_models[part_id] = {
        'model': model_part,
        'X_mean': X_mean,
        'X_std': X_std,
        'y_mean': y_mean,
        'y_std': y_std
    }

    print("í•™ìŠµ ì™„ë£Œ :", part_id)

# ğŸ”¹ ì˜ˆì¸¡ ìˆ˜í–‰ ë° ì„±ëŠ¥ í‰ê°€
# for part_id in list(part_models.keys()):
for part_id in part_models:
    # ë¶€í’ˆë³„ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    part_model_info = part_models[part_id]
    model = part_model_info['model']
    X_mean = part_model_info['X_mean']
    X_std = part_model_info['X_std']
    y_mean = part_model_info['y_mean']
    y_std = part_model_info['y_std']

    # ë¶€í’ˆë³„ ë°ì´í„° ì¤€ë¹„
    part_data = sales_data[sales_data['part_id'] == part_id]
    X_part = part_data[['month', 'year', 'Exchange_rate', 'sales_quantity', 'Hyundai_Month_Stock_Average']]
    y_part = part_data['sales_quantity'].shift(-1).dropna()
    X_part = X_part[:-1]  # Remove the last row to align with y_part

    # Normalize data
    X_part_normalized = (X_part - X_mean) / X_std
    y_part_normalized = (y_part - y_mean) / y_std

    # ì˜ˆì¸¡ ìˆ˜í–‰
    predictions_normalized = model.predict(X_part_normalized)
    predictions = predictions_normalized * y_std + y_mean

    # ì„±ëŠ¥ í‰ê°€
    mae = mean_absolute_error(y_part, predictions)
    mse = mean_squared_error(y_part, predictions)
    r2 = r2_score(y_part, predictions)

    # ì„±ëŠ¥ ì¶œë ¥
    print(f"ğŸ“Š {part_id} ë¶€í’ˆì˜ ì˜ˆì¸¡ ì„±ëŠ¥:")
    print(f"ğŸ“Š Mean Absolute Error (MAE): {mae:.2f}")
    print(f"ğŸ“Š Mean Squared Error (MSE): {mse:.2f}")
    print(f"ğŸ“Š R2 Score: {r2:.2f}\n")

# FastAPI app setup
app = FastAPI()

# API input model
class PredictRequest(BaseModel):
    part_id: str
    current_sales: int

def fetch_exchange_rate():
    response = requests.get("https://m.search.naver.com/p/csearch/content/qapirender.nhn?key=calculator&pkid=141&q=%ED%99%98%EC%9C%A8&where=m&u1=keb&u6=standardUnit&u7=0&u3=USD&u4=KRW&u8=down&u2=1")
    if response.status_code == 200:
        try:
            data = response.json()
            # Debug: Print the response to check the actual structure
            # print("API Response:", data)

            # Safely retrieve the exchange rate from the 'country' key
            if 'country' in data and len(data['country']) > 1:
                exchange_rate = float(data['country'][1]['value'].replace(",", ""))
                # print("ì‹¤ì‹œê°„ í™˜ìœ¨ (USD)",exchange_rate)
                return exchange_rate
            else:
                raise HTTPException(status_code=500, detail="Unexpected response structure")
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error parsing exchange rate: {str(e)}")
    else:
        raise HTTPException(status_code=500, detail="Failed to fetch exchange rate")

# Fetch Hyundai stock price
def fetch_hyundai_stock_price():
    # ì €ì¥ëœ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ì„ ê°€ì ¸ì˜¤ê¸°
    if os.path.exists("last_update.txt"):
        print("ì£¼ì‹ ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
        with open("last_update.txt", "r") as file:
            last_update = file.read().strip()
            last_update = datetime.strptime(last_update, '%Y-%m-%d')

        # í˜„ì¬ ë‚ ì§œê°€ ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ë‚ ì§œì™€ ê°™ì€ì§€ í™•ì¸
        if datetime.now().date() == last_update.date():
            # ì´ë¯¸ ì˜¤ëŠ˜ ì£¼ì‹ ê°€ê²©ì„ ê°€ì ¸ì™”ìœ¼ë©´ ì €ì¥ëœ ê°’ ì‚¬ìš©
            with open("stock_price.json", "r") as file:
                stock_data = json.load(file)
                print("ì´ë¯¸ ìˆëŠ” ê°’ ì‚¬ìš©",stock_data["price"])
                return stock_data["price"]
    else:
        print("ì£¼ì‹ ê°’ì„ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤.")
        last_update = datetime.min  # ì²˜ìŒ ì‹¤í–‰ ì‹œ ê¸°ë³¸ ë‚ ì§œ ì„¤ì •

    # ì£¼ì‹ ê°€ê²© ê°€ì ¸ì˜¤ê¸°
    for i in range(10):
        target_date = (datetime.now() - timedelta(days=i + 1)).strftime('%Y%m%d')
        params = {
            'serviceKey': SERVICE_KEY,
            'resultType': 'json',
            'numOfRows': '1',
            'pageNo': '1',
            'basDt': target_date,
            'likeSrtnCd': '005380'
        }

        response = requests.get(
            "http://apis.data.go.kr/1160100/service/GetStockSecuritiesInfoService/getStockPriceInfo", params=params,
            verify=certifi.where())

        if response.status_code == 200:
            data = response.json()
            items = data['response']['body']['items']['item']
            if items:
                # ì¢…ê°€ ì €ì¥
                stock_price = float(items[0]['clpr'])

                # ì˜¤ëŠ˜ì˜ ì£¼ì‹ ê°€ê²©ì„ stock_price.jsonì— ì €ì¥
                print("ì˜¤ëŠ˜ì˜ ì£¼ì‹ ê°€ê²©ì„ stock_price.jsonì— ì €ì¥",stock_price)
                with open("stock_price.json", "w") as file:
                    json.dump({"price": stock_price}, file)

                # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ì„ ê¸°ë¡
                print("ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ì„ ê¸°ë¡",datetime.now().strftime('%Y-%m-%d'))
                with open("last_update.txt", "w") as file:
                    file.write(datetime.now().strftime('%Y-%m-%d'))

                return stock_price
        else:
            raise HTTPException(status_code=500, detail="Failed to fetch stock price")

    # ì„¸ ë²ˆì˜ ë‚ ì§œì— ëŒ€í•´ ëª¨ë‘ ê°€ê²©ì„ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ 404 ì—ëŸ¬ ë°œìƒ
    raise HTTPException(status_code=404, detail="Stock price not found for the past 10 days")

@app.get("/")
def read_root():
    return {"message": "ë¦¼ë™ì—°(AIìœµí•©í•™ë¶€ ì¶œì‹ )ë‹˜ì´ ì–´ë µê²Œ ë§Œë“  AI ì„œë²„ì…ë‹ˆë‹¤.\në¬´ë‹¨ ì‚¬ìš©ì„ ì—„í•˜ê²Œ ê¸ˆì§€í•©ë‹ˆë‹¤."}

@app.post("/predict")
def predict_sales(request: PredictRequest):
    # ğŸ”¹ part_idë¡œ í•´ë‹¹ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
    if request.part_id not in part_models:
        raise HTTPException(status_code=404, detail=f"ğŸš« '{request.part_id}'ì— ëŒ€í•œ ëª¨ë¸ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    part_model_info = part_models[request.part_id]
    model = part_model_info['model']
    X_mean = part_model_info['X_mean']
    X_std = part_model_info['X_std']
    y_mean = part_model_info['y_mean']
    y_std = part_model_info['y_std']

    current_exchange_rate = fetch_exchange_rate()
    current_stock_price = fetch_hyundai_stock_price()
    current_date = pd.Timestamp.now()

    # ğŸ”¹ ì…ë ¥ ë°ì´í„° ì •ê·œí™”
    input_data = pd.DataFrame({
        'month': [current_date.month],
        'year': [current_date.year],
        'Exchange_rate': [current_exchange_rate],
        'sales_quantity': [request.current_sales],
        'Hyundai_Month_Stock_Average': [current_stock_price]
    })

    input_data_normalized = (input_data - X_mean) / X_std

    # ğŸ”¹ ì˜ˆì¸¡ ìˆ˜í–‰
    predictions_normalized = model.predict(input_data_normalized)[0][0]
    predicted_sales = predictions_normalized * y_std + y_mean

    return {
        'part_id': request.part_id,
        'current_month_sales': request.current_sales,
        'predicted_next_month_sales': max(10, int(predicted_sales)),
    }




# from fastapi import FastAPI, HTTPException
# from pydantic import BaseModel
# import tensorflow as tf
# import pandas as pd
# import numpy as np
# import requests
#
# # Load data
# sales_data = pd.read_csv('mock_sales_data.csv')
# exchange_data = pd.read_csv('exchange.csv')
# hyundai_car_stock_data = pd.read_csv('hyundai_stock_monthly_avg.csv')
#
# # Data preprocessing
# sales_data['date'] = pd.to_datetime(sales_data['date'], format='%Y-%m')
# exchange_data['date'] = pd.to_datetime(exchange_data[['Year', 'Month']].assign(Day=1))
#
# # Merge data
# sales_data = sales_data.merge(exchange_data, how='left', on='date')
# sales_data.fillna(method='ffill', inplace=True)  # Fill missing exchange rates
#
# # Create features and labels for training
# sales_data['month'] = sales_data['date'].dt.month
# sales_data['year'] = sales_data['date'].dt.year
#
# X = sales_data[['month', 'year', 'Exchange_rate', 'sales_quantity']]
# y = sales_data['sales_quantity'].shift(-1).dropna()  # Predict next month's sales
# X = X[:-1]  # Remove the last row to match y length
#
# # Normalize data
# X_mean, X_std = X.mean(), X.std()
# X_normalized = (X - X_mean) / X_std
# y_mean, y_std = y.mean(), y.std()
# y_normalized = (y - y_mean) / y_std
#
# # TensorFlow model
# model = tf.keras.Sequential([
#     tf.keras.layers.Dense(10, activation='relu', input_shape=(X_normalized.shape[1],)),
#     tf.keras.layers.Dense(1)
# ])
#
# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])
#
# # Train the model
# model.fit(X_normalized, y_normalized, epochs=20, batch_size=10, verbose=1, validation_split=0.2)
#
# # FastAPI app setup
# app = FastAPI()
#
# # API input model
# class PredictRequest(BaseModel):
#     part_id: str
#     current_sales: int  # Input the current month's sales
#
# # Function to fetch real-time exchange rate
# def fetch_exchange_rate():
#     response = requests.get("https://m.search.naver.com/p/csearch/content/qapirender.nhn?key=calculator&pkid=141&q=%ED%99%98%EC%9C%A8&where=m&u1=keb&u6=standardUnit&u7=0&u3=USD&u4=KRW&u8=down&u2=1")
#     if response.status_code == 200:
#         try:
#             data = response.json()
#             # Debug: Print the response to check the actual structure
#             print("API Response:", data)
#
#             # Safely retrieve the exchange rate from the 'country' key
#             if 'country' in data and len(data['country']) > 1:
#                 exchange_rate = float(data['country'][1]['value'].replace(",", ""))
#                 return exchange_rate
#             else:
#                 raise HTTPException(status_code=500, detail="Unexpected response structure")
#         except Exception as e:
#             raise HTTPException(status_code=500, detail=f"Error parsing exchange rate: {str(e)}")
#     else:
#         raise HTTPException(status_code=500, detail="Failed to fetch exchange rate")
#
#
# @app.get("/")
# async def read_root():
#     return {"message": "Hello World"}
#
# # # 1ì°¨ í•¨ìˆ˜ ëª¨ë¸ ì •ì˜
# # class LinearModel(tf.Module):
# #     def __call__(self, x):
# #         return tf.constant(x + 1)  # í…ì„œë¡œ ë°˜í™˜
# #
# # model = LinearModel()
# #
# # # API ì—”ë“œí¬ì¸íŠ¸ ì •ì˜
# # @app.get("/predict/{x}")
# # def predict(x: int):
# #     # ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡
# #     y = model(x)
# #     return {"x": x, "y": y.numpy().tolist()}  # yë¥¼ í…ì„œë¡œ ì²˜ë¦¬
#
#
#
# @app.post("/predict")
# async def predict_sales(request: PredictRequest):
#     # Validate part ID
#     part_sales_data = sales_data[sales_data['part_id'] == request.part_id]
#     if part_sales_data.empty:
#         raise HTTPException(status_code=404, detail="Part ID not found")
#
#     # Fetch the real-time exchange rate
#     current_exchange_rate = fetch_exchange_rate()
#     current_date = pd.Timestamp.now()
#
#     # Prepare input data
#     input_data = pd.DataFrame({
#         'month': [current_date.month],
#         'year': [current_date.year],
#         'Exchange_rate': [current_exchange_rate],
#         'sales_quantity': [request.current_sales]
#     })
#     input_data = (input_data - X_mean) / X_std  # Normalize input
#
#     # Predict the next month's sales
#     predicted_sales = model.predict(input_data)[0][0]
#     predicted_sales = predicted_sales * y_std + y_mean  # Denormalize the prediction
#
#     return {
#         'part_id': request.part_id,
#         'current_month_sales': request.current_sales,
#         'predicted_next_month_sales': max(10, int(predicted_sales)),
#     }
